import * as mediasoupClient from "mediasoup-client";
import { useEffect, useRef } from "react";
import { io } from "socket.io-client";
import { socket } from "../lib/socket";

export function useVoiceChat(roomId, member, onSpeakingUsersChange) {
  // const [volume, setVolume] = useState(0);
  const analyserRef = useRef(null);
  const dataArrayRef = useRef(null);
  const speakingRef = useRef(false);
  const animationIdRef = useRef(null);
  const streamRef = useRef(null);
  const socketRef = useRef(null);
  const deviceRef = useRef(null);
  const sendTransportRef = useRef(null);
  const recvTransportsRef = useRef([]);
  const audioElementsRef = useRef({});
  // const audioContextRef = useRef(null);

  useEffect(() => {
    if (!roomId || !member) return;

    socketRef.current = socket;

    const start = async () => {
      if (!member) return;
      // 1. ì±„ë„ ìž…ìž¥
      // socket.on("joinRoom", ({ roomId, member }) => {
      //   console.log(`ðŸ”” Room Joined: ${roomId} by ${socket.id}`);
      //   console.log("âž¡ï¸ member ì •ë³´:", member);
      socket.emit("joinRoom", {
        roomId,
        member: {
          memberId: member.mno,
          name: member.name,
          profile: member.profile || "",
        },
      });

      // 2. ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¼ íšë“
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      console.log("ðŸŽ™ï¸ ë§ˆì´í¬ íŠ¸ëž™:", stream.getAudioTracks());
      const audioTrack = stream.getAudioTracks()[0];

      // 3. speaking ê°ì§€ ì¤€ë¹„
      const audioContext = new (window.AudioContext || window.webkitAudioContext)();
      const analyser = audioContext.createAnalyser();
      const micSource = audioContext.createMediaStreamSource(stream);
      micSource.connect(analyser);
      analyser.fftSize = 256;
      const dataArray = new Uint8Array(analyser.frequencyBinCount);
      analyserRef.current = analyser;
      dataArrayRef.current = dataArray;

      // 4. RTP Capabilities ìš”ì²­ â†’ device ìƒì„±
      socket.emit("getRtpCapabilities", null, async (rtpCapabilities) => {
        const device = new mediasoupClient.Device();
        await device.load({ routerRtpCapabilities: rtpCapabilities });
        console.log("âœ… rtpCapabilities ìˆ˜ì‹ :", rtpCapabilities);
        deviceRef.current = device;
        console.log(device);
        // 5. ì†¡ì‹  Transport ìƒì„±
        socket.emit("createTransport", async (params) => {
          const sendTransport = device.createSendTransport(params);
          console.log("ì—¬ê¸°ëŠ” ë˜ê²ƒì§€?");
          sendTransportRef.current = sendTransport; // ì´ê±° ìœ„ì¹˜ëŠ” ìƒê´€ì—†ë‚˜?

          sendTransport.on("connect", ({ dtlsParameters }, callback) => {
            socket.emit("connectTransport", { dtlsParameters });
            console.log("ì—°ê²° ì™„ë£Œ");
            callback();
          });

          sendTransport.on("produce", ({ kind, rtpParameters }, callback) => {
            socket.emit("produce", { kind, rtpParameters }, ({ id }) => callback({ id }));
            console.log("ðŸŽ¤ ì˜¤ë””ì˜¤ íŠ¸ëž™ ë“±ë¡ ì™„ë£Œ");
          });

          await sendTransport.produce({ track: audioTrack });
        });
      });

      // 6. ì†Œë¹„ìž ìˆ˜ì‹  ì²˜ë¦¬
      socket.on("newProducer", async ({ producerId, socketId }) => {
        console.log("ðŸ†• ìƒˆë¡œìš´ producer ìˆ˜ì‹ :", producerId, socketId);
        const device = deviceRef.current;
        if (!device) return;
        // ìˆ˜ì‹ ìš© íŠ¸ëžœìŠ¤í¬íŠ¸ ìš”ì²­
        socket.emit("createRecvTransport", async (params) => {
          const recvTransport = device.createRecvTransport(params);

          recvTransport.on("connect", ({ dtlsParameters }, callback) => {
            socket.emit("connectRecvTransport", {
              dtlsParameters,
              transportId: recvTransport.id,
            });
            callback();
          });

          // consume ìš”ì²­
          socket.emit(
            "consume",
            {
              rtpCapabilities: device.rtpCapabilities,
              producerSocketId: socketId,
              // transportId: recvTransport.id,
            },
            async ({ id, kind, rtpParameters, producerId }) => {
              const consumer = await recvTransport.consume({
                id,
                producerId,
                kind,
                rtpParameters,
              });

              const stream = new MediaStream([consumer.track]);
              const audio = new Audio();
              audio.srcObject = stream;
              audio.autoplay = true;
              console.log("ðŸ”Š consumer ìƒì„±ë¨", consumer);
              console.log("ðŸ”ˆ stream ìƒì„±ë¨", stream);
              audio
                .play()
                .then(() => {
                  console.log("âœ… ì˜¤ë””ì˜¤ ìž¬ìƒ ì‹œìž‘ë¨");
                })
                .catch((err) => {
                  console.error("ðŸ”‡ ì˜¤ë””ì˜¤ ìž¬ìƒ ì‹¤íŒ¨", err);
                });

              audioElementsRef.current[producerId] = audio;
              recvTransportsRef.current.push(recvTransport);
            }
          );
        });
      });
      // ðŸ”Š ë³¼ë¥¨ ì‹œê°í™”
      // const audioContext = new (window.AudioContext || window.webkitAudioContext)();
      // const analyser = audioContext.createAnalyser();
      // const micSource = audioContext.createMediaStreamSource(stream);
      // micSource.connect(analyser);
      // analyser.fftSize = 256;
      // const dataArray = new Uint8Array(analyser.frequencyBinCount);

      // const updateVolume = () => {
      //   analyser.getByteFrequencyData(dataArray);
      //   const avg = dataArray.reduce((a, b) => a + b) / dataArray.length;
      //   setVolume(Math.round(avg));
      //   requestAnimationFrame(updateVolume);
      // };
      // updateVolume();
      // audioContextRef.current = audioContext;
      // 7. ë§í•˜ê¸° ê°ì§€ ë° emit
      const checkSpeaking = () => {
        analyser.getByteFrequencyData(dataArray);
        const avg = dataArray.reduce((a, b) => a + b) / dataArray.length;
        const isSpeaking = avg > 15;

        if (isSpeaking !== speakingRef.current) {
          speakingRef.current = isSpeaking;
          socket.emit("speaking", {
            roomId,
            memberId: member.memberId,
            speaking: isSpeaking,
          });
        }

        animationIdRef.current = requestAnimationFrame(checkSpeaking);
      };
      checkSpeaking();

      // 8. ë‹¤ë¥¸ ì‚¬ìš©ìž speaking ìˆ˜ì‹ 
      socket.on("speaking-users", (list) => {
        onSpeakingUsersChange?.(list);
      });
    };

    start();

    return () => {
      socket.emit("leaveRoom", roomId);
      socket.off("newProducer");
      socket.off("speaking-users");

      streamRef.current?.getTracks().forEach((t) => t.stop());
      sendTransportRef.current?.close();
      recvTransportsRef.current.forEach((t) => t.close());
      Object.values(audioElementsRef.current).forEach((a) => {
        a.srcObject?.getTracks().forEach((t) => t.stop());
        a.remove();
      });

      if (animationIdRef.current) cancelAnimationFrame(animationIdRef.current);

      // socketRef.current?.disconnect();
      // sendTransportRef.current?.close();
      // recvTransportsRef.current.forEach((t) => t.close());

      // Object.values(audioElementsRef.current).forEach((audio) => {
      //   audio.srcObject?.getTracks().forEach((track) => track.stop());
      //   audio.remove();
      // });
    };
  }, [roomId, member]);
  // return { volume };
  return {};
  // return {
  //   // ìˆ˜ë™ ì‚¬ìš©ì‹œ ì´ê±¸ë¡œ ë°”ê¾¸ë©´ ë¨
  //   startSpeaking: () => socket.emit("speaking", { roomId, memberId: member.memberId, speaking: true }),
  //   stopSpeaking: () => socket.emit("speaking", { roomId, memberId: member.memberId, speaking: false }),
  // };
}
